{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Business Problem:\n",
    "        \n",
    "What is pneumonia?<br>\n",
    "    Pneumonia is an inflammatory condition of the lung that mainly affects the small air sacs known as alveoli. Symptoms usually include some combination of a productive or dry cough, chest pain, fever, and difficulty breathing. The severity of the condition is variable. Pneumonia is usually caused by infection by viruses or bacteria and less commonly by other microorganisms, certain medications or conditions such as autoimmune diseases. Risk factors include cystic fibrosis, chronic obstructive pulmonary disease (COPD), asthma, diabetes, heart failure, poor ability to cough, as after a stroke and a weak immune system. The diagnosis is usually based on symptoms and physical examination. Chest radiography, blood and sputum culture can help confirm the diagnosis. The disease can be classified according to the place where it was acquired, such as community-acquired or hospital-acquired pneumonia or associated with healthcare.<br>\n",
    "    \n",
    "The business problem is dealing with lives, where a wrong prediction could be costing someone else's life. The goal is bring the highest possible precision to the model, analyzing models tirelessly until we create the best model possible. The goal in costume, it is obviously 100%, lives are priceless. However, Deep Learning is math and mistakes can happen, with that in mind. A target of at least 97% accuracy is set, with an ideal target of 98.5%. <br>\n",
    "        \n",
    "As input the model will receive an x-ray of the chest, where it will classify whether that person has pneumonia or not. <br>\n",
    "        \n",
    "Suggestions for Improvement:<br>\n",
    "    1 - Use Transfer-Learning (Insufficient Memory)<br>\n",
    "    2 - Use Tuning to find the best parameters (Using already in hyperparameters)<br>\n",
    "    3 - Use of larger images (Images above 248 make tests impossible)<br>\n",
    "    4 - Apply a new split, as it has a large imbalance between the datasets (Applied, observed significant improvement)<br>\n",
    "    5 - Pre-apply image filters<br>\n",
    "    6 - Reduce batch size (Batch size of 16 generated a gain in accuracy, but an increase in Loss.)<br>\n",
    "\n",
    "Note: For items 1 and 3, a computer with high availability by ram memory is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import cv2\n",
    "import os\n",
    "import type_models.utils as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading images and applying filters\n",
    "# Opencv grayscale applied and resized to IMG_SIZE\n",
    "def get_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            # Grayscale and image reshape\n",
    "            img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE)) # COMMENT FOR EXPLORATORY ANALYSIS\n",
    "            \n",
    "            data.append([resized_arr, class_num])\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add row to dataframe\n",
    "def add_row(df, row):\n",
    "    df.loc[-1] = row\n",
    "    df.index = df.index + 1  \n",
    "    return df.sort_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DEFINITION OF GLOBAL VARIABLES FOR ENVIRONMENTAL EXECUTION #######\n",
    "\n",
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "IMG_SIZE = 150\n",
    "BATCH_SIZE = 16\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the average size of the images, the following statement arrives: <br>\n",
    "    max_height = 2916 <br>\n",
    "    max_width = 2713 <br>\n",
    "    media_height = 1327 <br>\n",
    "    media_width = 970 <br>\n",
    "    min_height = 384 <br>\n",
    "    min_width = 127 <br>\n",
    "However, viewing through a histogram, it is stated that the mean is not representative,\n",
    "thus, several tests were carried out, until the IMG_SIZE was reached between 128 and 250.\n",
    "After analyzing some cases, 150 were adopted, since 250 were at risk of decreasing\n",
    "accuracy without increasing the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data('Dataset/chest_xray/train')\n",
    "test = get_data('Dataset/chest_xray/test')\n",
    "val = get_data('Dataset/chest_xray/val')\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is separated into vectors with the images and labels, according to its definition\n",
    "for array_image, label in train:\n",
    "    x_train.append(array_image)\n",
    "    y_train.append(label)\n",
    "\n",
    "for array_image, label in test:\n",
    "    x_test.append(array_image)\n",
    "    y_test.append(label)\n",
    "    \n",
    "for array_image, label in val:\n",
    "    x_val.append(array_image)\n",
    "    y_val.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the graph below, it can be seen that the dataset is totally unbalanced, has many more images of Pneumonia, for better performance it is necessary apply a new distribution on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        l.append(\"Pneumonia\")\n",
    "    else:\n",
    "        l.append(\"Normal\")\n",
    "sns.countplot(l) \n",
    "plt.savefig('count_normal_pneumo.png', format='png')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better analysis, all exploratory analysis coding was separated in the utils.py file, it is recommended to read the utils.py file to understand the results and what they mean. <br> <br>\n",
    "    \n",
    "Brief analysis: <br>\n",
    "The main analysis was performed on the measures of width and height, this evaluation was carried out,\n",
    "in order to reach an IMG_SIZE with greater representativeness. It was evaluated by the Histogram,\n",
    "and Quartiles where there should be a greater focus. The box plot served as an analysis, to identify,\n",
    "in general the view of the dataset. <br>\n",
    "A big problem with the analysis is that the informed outputs brought results that were difficult to simulate,\n",
    "arriving at times when an implementation would become unviable or exhaustive. <br>\n",
    "The suggestion for training more complex models is to rent Clusters from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_total = x_train + x_val + x_test\n",
    "\n",
    "# Performed the analysis for each split, after pre-processing can still be performed again for a new comparison\n",
    "\n",
    "'''\n",
    "u.main(x_train)\n",
    "u.main(x_val)\n",
    "u.main(x_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the inconsistency in the dataset, it is analyzed that a new split can help in the performance. The performance had an average increase of 5% in the analyzes performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW SPLIT ###\n",
    "\n",
    "# Joining all the data in the old list format in a single list of total x and y\n",
    "x_total = x_train + x_val + x_test\n",
    "y_total = y_train + y_val + y_test\n",
    "\n",
    "# Converting the total list to data frame and filling in\n",
    "total_df = pd.DataFrame(columns = ['Img', 'Label'])\n",
    "\n",
    "for i in range(len(x_total)):\n",
    "    add_row(total_df, [x_total[i], y_total[i]])\n",
    "\n",
    "# Separating into training and test data\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(total_df.drop('Label',axis=1),\n",
    "                                                    total_df['Label'],                             \n",
    "                                                    test_size = 0.3)\n",
    "\n",
    "# Separating the data again between testing and validation\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.05)\n",
    "\n",
    "# Dataframe to list conversion performed again\n",
    "x_train = x_train['Img'].values.tolist()\n",
    "x_val = x_val['Img'].values.tolist()\n",
    "x_test = x_test['Img'].values.tolist()\n",
    "\n",
    "y_train = y_train.values.tolist()\n",
    "y_val = y_val.values.tolist()\n",
    "y_test = y_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel normalization is performed for faster execution\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "x_test = np.array(x_test) / 255\n",
    "\n",
    "# Resizing is applied over the array, to a format that Keras accepts\n",
    "x_train = x_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val = x_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "x_test = x_test.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation is used to have an outlier reduction and greater precision\n",
    "datagen = ImageDataGenerator(rotation_range = 30, zoom_range = 0.2, width_shift_range = 0.1, height_shift_range = 0.1, \n",
    "                             horizontal_flip = True)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolutional Neural Network is created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network below was created in a block structure:<br>\n",
    "Conv2D - Each block has a Dense layer, to recalculate the weights from filters, to each layer\n",
    "more filters are applied, so we can have more precision in the pixels.<br>\n",
    "    \n",
    "BatchNormalization - After the new calculations, a new normalization of the output values is performed,\n",
    "so we maintain efficiency in calculations and improving accuracy.<br>\n",
    "    \n",
    "MaxPooling2D - Used to prioritize the highest output values, as these have the greatest impact.<br>\n",
    "    \n",
    "Dropout - To avoid the outlier, Dropout is applied where values that differ many of the others are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First Block (Input)\n",
    "model.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = INPUT_SHAPE))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Second Block\n",
    "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Third Block\n",
    "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Fourth Block\n",
    "model.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Fifth Blocks\n",
    "model.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Sixth Block - Flatten is used to convert pixels from matrix to array\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128 , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build using RMSPROP or ADAM\n",
    "model.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# A callback is created to reduce the training rate and we don't fall in minimum places\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1, factor=0.3, min_lr = 0.000001)\n",
    "\n",
    "\n",
    "\n",
    "# The model is trained\n",
    "model_history = model.fit(datagen.flow(x_train,y_train, batch_size = BATCH_SIZE), epochs = 13,\n",
    "                    validation_data = datagen.flow(x_val, y_val), callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is applied on the test basis to measure its accuracy\n",
    "print(\"Loss: \" , model.evaluate(x_test,y_test)[0])\n",
    "print(\"Accuracy: \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the evolution of accuracy throughout the times\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of loss over time\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction value is stored\n",
    "predictions = model.predict_classes(x_test)\n",
    "predictions = predictions.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data about the model is displayed:<br>\n",
    "        \n",
    "Accuracy: The hit rate minus error, placed in %<br>\n",
    "Recall: The rate of hit frequency, how much do you hit each error?<br>\n",
    "f1-score: F1 is the combination of precision and recall in a single score.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions, target_names = ['Pneumonia (Class 0)','Normal (Class 1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The precision matrix is created to visualize the number of correct cases\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])   \n",
    " \n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='', xticklabels = labels, yticklabels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is saved its shape and weights, for future use\n",
    "model_json = model.to_json()\n",
    "model_name = 'classifier'\n",
    "with open(\"models/complexity_model_advanced/\" + model_name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"models/complexity_model_advanced/\" + model_name + \".h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
